{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e378e495",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4970ad",
   "metadata": {},
   "source": [
    "## Q1. What is the KNN algorithm?\n",
    "K-Nearest Neighbors (KNN) is a lazy learning and non-parametric algorithm used for both classification and regression tasks. It works by finding the K nearest data points in the training set to a given input and making predictions based on those neighbors. For classification, it assigns the class that is most common among the neighbors. For regression, it predicts the average of the neighbors' values.\n",
    "\n",
    "## Q2. How do you choose the value of K in KNN?\n",
    "Choosing the value of K in KNN involves a tradeoff:\n",
    "\n",
    "A small K (e.g., K=1) makes the model sensitive to noise and may lead to overfitting.\n",
    "A large K makes the model smoother, reducing variance but potentially increasing bias.\n",
    "A common method to choose K is through cross-validation, where you test different K values and select the one that minimizes the validation error. Typically, K is chosen as an odd number to avoid ties in classification tasks.\n",
    "\n",
    "## Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "KNN Classifier: In a classification problem, the algorithm assigns the label of the majority class among the K nearest neighbors.\n",
    "KNN Regressor: In a regression problem, the algorithm predicts the output as the average (or weighted average) of the target values of the K nearest neighbors.\n",
    "The key difference lies in how the predictions are made: classification uses voting, while regression uses averaging.\n",
    "\n",
    "## Q4. How do you measure the performance of KNN?\n",
    "For classification:\n",
    "\n",
    "Accuracy: The percentage of correctly classified instances.\n",
    "Precision, Recall, and F1-score: Useful for imbalanced classes.\n",
    "Confusion Matrix: To visualize performance across different classes.\n",
    "For regression:\n",
    "\n",
    "Mean Squared Error (MSE): Measures the average squared difference between predicted and actual values.\n",
    "Mean Absolute Error (MAE): The average absolute difference between predicted and actual values.\n",
    "R-squared: Represents the proportion of variance explained by the model.\n",
    "\n",
    "## Q5. What is the curse of dimensionality in KNN?\n",
    "The curse of dimensionality refers to the phenomenon where the performance of KNN degrades as the number of features (dimensions) increases. In higher-dimensional spaces, the data becomes sparse, and the concept of distance becomes less meaningful. This makes it difficult to find nearest neighbors that are truly \"close\" in terms of distance.\n",
    "\n",
    "## Q6. How do you handle missing values in KNN?\n",
    "Common strategies to handle missing values in KNN include:\n",
    "\n",
    "Imputation: Replace missing values with the mean, median, or mode of the respective feature.\n",
    "KNN Imputation: Use KNN to impute missing values by finding the K nearest neighbors (based on available features) and imputing the missing value using the mean or mode of the neighbors.\n",
    "Removing missing data: If the percentage of missing values is small, you may choose to drop those rows.\n",
    "\n",
    "## Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "KNN Classifier: Best suited for classification tasks where decision boundaries between classes are well-defined and the number of classes is manageable. It can work well in low-dimensional spaces with clear class separation.\n",
    "\n",
    "KNN Regressor: Best for regression tasks with smooth variations in the target variable. It is more sensitive to the choice of K and distance metric in cases where outliers or noise exist.\n",
    "\n",
    "Both models tend to struggle with high-dimensional data due to the curse of dimensionality. KNN classifiers are typically better for problems where the decision boundary is simple and non-linear, while KNN regressors can work well for smooth, continuous functions but might suffer from high bias or variance depending on K.\n",
    "\n",
    "## Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "Strengths:\n",
    "\n",
    "Simple and intuitive: Easy to implement and understand.\n",
    "No training phase: All computations are done at the time of prediction.\n",
    "Non-parametric: No assumptions about the distribution of the data.\n",
    "Weaknesses:\n",
    "\n",
    "Computationally expensive: Since KNN makes predictions by scanning the entire training set, it is slow with large datasets.\n",
    "Curse of dimensionality: As the number of features increases, the performance degrades.\n",
    "Sensitive to irrelevant features: KNN heavily relies on the distance metric, so irrelevant or unscaled features can negatively affect performance.\n",
    "Addressing weaknesses:\n",
    "\n",
    "Dimensionality reduction: Use techniques like PCA to reduce the number of features.\n",
    "Feature scaling: Standardize or normalize the data to improve performance.\n",
    "Efficient nearest neighbor search: Use algorithms like KD-Trees or Ball Trees for faster predictions.\n",
    "\n",
    "## Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "Euclidean distance: Measures the straight-line distance between two points in Euclidean space. It is the most commonly used distance metric in KNN and is calculated as:\n",
    "\n",
    "𝑑\n",
    "(\n",
    "𝑝\n",
    ",\n",
    "𝑞\n",
    ")\n",
    "=\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "(\n",
    "𝑝\n",
    "𝑖\n",
    "−\n",
    "𝑞\n",
    "𝑖\n",
    ")\n",
    "2\n",
    "d(p,q)= \n",
    "i=1\n",
    "∑\n",
    "n\n",
    "​\n",
    " (p \n",
    "i\n",
    "​\n",
    " −q \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " \n",
    "​\n",
    " \n",
    "Manhattan distance: Measures the distance between two points along the axes at right angles, also known as \"city block\" distance. It is calculated as:\n",
    "\n",
    "𝑑\n",
    "(\n",
    "𝑝\n",
    ",\n",
    "𝑞\n",
    ")\n",
    "=\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "∣\n",
    "𝑝\n",
    "𝑖\n",
    "−\n",
    "𝑞\n",
    "𝑖\n",
    "∣\n",
    "d(p,q)= \n",
    "i=1\n",
    "∑\n",
    "n\n",
    "​\n",
    " ∣p \n",
    "i\n",
    "​\n",
    " −q \n",
    "i\n",
    "​\n",
    " ∣\n",
    "Manhattan distance is often used when the data is sparse or when features have distinct grid-like properties, while Euclidean distance is preferred when the relationship between features is continuous.\n",
    "\n",
    "## Q10. What is the role of feature scaling in KNN?\n",
    "Feature scaling is essential in KNN because the algorithm relies on the distance between data points. If the features are not scaled to a common range, features with larger ranges will dominate the distance calculation, leading to biased results. Common methods for feature scaling include:\n",
    "\n",
    "Min-max scaling: Rescaling features to a range of [0, 1].\n",
    "Standardization: Scaling features to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438bc1de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
